//  CopyriaAght (c) 2011-present, Facebook, Inc.  All rights reserved.
//  This source code is licensed under the BSD-style license found in the
//  LICENSE file in the root directory of this source tree. An additional grant
//  of patent rights can be found in the PATENTS file in the same directory.

#include <algorithm>
#include <atomic>
#include <cstdlib>
#include <iterator>
#include <memory>
#include <queue>
#include <utility>
#include <vector>

// c
#include <terark/heap_ext.hpp>
#include <terark/mempool.hpp>
#include <terark/stdtypes.hpp>
#include <terark/thread/instance_tls.hpp>
// C

#include "db/memtable.h"
#include "rocksdb/memtablerep.h"
#include "util/arena.h"
#include "util/mutexlock.h"
#include "util/threaded_rbtree.h"

namespace rocksdb {
namespace {

class ThreadedRBTreeRep : public MemTableRep {
 private:
  template <size_t padding = 2>
  class rbtree_impl : public std::enable_shared_from_this<rbtree_impl<padding>> {
   protected:
    using my_mempool_t = terark::MemPool<1ull << padding>;

    template <class index_t, class key_t>
    struct trbt_node_t {
      typedef typename index_t index_type;
      typedef typename key_t key_type;

      static const index_type nil_sentinel = index_type(-1);

      index_type lch = nil_sentinel;
      index_type rch = nil_sentinel;
      uint8_t clr : 1;
      uint8_t lcT : 1;
      uint8_t rcT : 1;
      uint8_t kln : 5;
      char datum[1];

      bool is_black() const { return !clr; }
      bool is_red() const { return clr; }
      bool left_is_child() const { return !lcT; }
      bool left_is_thread() const { return lcT; }
      bool right_is_child() const { return !rcT; }
      bool right_is_thread() const { return rcT; }

      void set_used() {}
      void set_black() { clr = 0; }
      void set_red() { clr = 1; }
      void left_set_child() { lcT = 0; }
      void left_set_thread() { lcT = 1; }
      void right_set_child() { rcT = 0; }
      void right_set_thread() { rcT = 1; }

      index_type left_get_link() { return lch; }
      index_type right_get_link() { return rch; }
      void left_set_link(index_type l) { lch = l; }
      void right_set_link(index_type r) { rch = r; }

      key_type key() const {
        if (kln < 31)
          return key_type{datum, size_t(kln) + 8};
        else {
          uint32_t ret = 0;
          auto p = GetVarint32Ptr(datum, datum + 8, &ret);
          return key_type{p, ret + 39};
        }
      }

      key_type value() const {
        auto k = key();
        auto p = k.data() + k.size();
        uint32_t vlen;
        p = GetVarint32Ptr(p, p + 8, &vlen);
        return key_type(p, vlen);
      }

      void set_key_value(const key_type &key, const key_type &val,
                         uint32_t klnln, uint32_t vlnln, bool mag) {
        char *p = datum;
        assert((key.size() < 39) ^ mag);
        kln = mag ? 31 : key.size() - 8;
        if (mag) {
          EncodeVarint32(p, static_cast<uint32_t>(key.size() - 39));
          p += klnln;
        }
        memcpy(p, key.data(), key.size());
        p += key.size();
        EncodeVarint32(p, static_cast<uint32_t>(val.size()));
        p += vlnln;
        memcpy(p, val.data(), val.size());
      }

      size_t capicity() {
        auto v = value();
        auto p = v.data() + v.size();
        return p - datum + 9;
      }
    };

    template <class node_t>
    struct trbt_deref_node_t {
      my_mempool_t *handle;
      node_t &operator()(typename node_t::index_type idx) const {
        return handle->at<node_t>(idx << padding);
      }
    };

    template <class node_t>
    struct trbt_deref_key_t {
      my_mempool_t *handle;
      typename const node_t::key_type operator()(
          typename node_t::index_type idx) const {
        return handle->at<node_t>(idx << padding).key();
      }
    };

    template <class node_t, size_t max_depth>
    struct trbt_rep_stack_t {
      typename typedef node_t::index_type index_t;

      size_t height = 0, path = 0;
      index_t offsets[max_depth];

      bool is_left(size_t k) {
        assert(k < max_depth);
        return !((path >> k) & 1);
      }

      bool is_right(size_t k) {
        assert(k < max_depth);
        return (path >> k) & 1;
      }

      index_t get_index(size_t k) {
        assert(k < max_depth);
        return offsets[k];
      }

      void push_index(index_t index, bool left) {
        if (height == max_depth) {
          throw std::length_error("thread_rbtree_stack overflow");
        }
        if (left)
          path &= ~(1ull << height);
        else
          path |= (1ull << height);
        offsets[height++] = index;
      }

      void update_index(size_t k, index_t index, bool left) {
        assert(k < max_depth);
        offsets[k] = index_type(index);
        if (left)
          path &= ~(1 << k);
        else
          path |= (1 << k);
      }

      void update_index(size_t k, index_t index) {
        assert(k < max_depth);
        offsets[k] = index_type(mask_type(index));
      }
    };

    template <class deref_t>
    struct trbt_rep_cmp_t {
      bool operator()(size_t x, size_t y) const { return compare(x, y) < 0; }
      bool operator()(const Slice &x, const Slice &y) const {
        return compare(x, y) < 0;
      }
      int compare(size_t x, size_t y) const {
        return c->Compare(deref(x), deref(y));
      }
      int compare(const Slice &x, const Slice &y) const {
        return c->Compare(x, y);
      }
      deref_t deref;
      const InternalKeyComparator *c;
    };

    using my_index_t = uint32_t;
    using my_key_t = rocksdb::Slice;
    using my_node_t = trbt_node_t<my_index_t, my_key_t>;
    using my_deref_node_t = trbt_deref_node_t<my_node_t>;
    using my_deref_key_t = trbt_deref_key_t<my_node_t>;
    using my_cmp_t = trbt_rep_cmp_t<my_deref_key_t>;
    using my_stack_t = trbt_rep_stack_t<my_node_t, 64>;
    using my_root_t =
        threaded_rbtree_root_t<my_node_t, std::true_type, std::true_type>;

   private:
    my_mempool_t data_;
    my_root_t root_;
    size_t item_num_ = 0;
    mutable port::RWMutex mutex_;
    bool readOnly_ = false;
    ThreadedRBTreeRep *rep_;
    static const size_t lowCapicity = 64 * 1024 * 1024;

   public:
    rbtree_impl(ThreadedRBTreeRep *rep) : data_(256), rep_(rep) {}

    void Destory() {
      size_t mem_used = data_.size();
      if (mem_used < lowCapicity) {
        WriteLock rep_lock(&rep_->mutex_);
        auto &list = rep_->rbt_list_;
        for (auto curr = list.begin(), self = list.end(); curr != list.end(); curr++) {
          auto curr_ptr = &**curr;
          if (this == curr_ptr)
            self = curr;
          else if ((curr_ptr->MemUsed() + mem_used) / 4 - UINT32_MAX >= 0) {
            WriteLock lock(&curr_ptr->mutex_);
            if ((curr_ptr->MemUsed() + mem_used) / 4 - UINT32_MAX >= 0) {
              if (self == list.end()) {
                for (self = std::next(curr); this != &**self; ++self)
                  ;
              }
              curr_ptr->Merge(&**self);
              list.erase(self);
              break;
            }
          }
        }
      }
    }

    ThreadedRBTreeRep *Rep() { return rep_; }

    template <class Lock>
    class InnerIterator {
     private:
      std::shared_ptr<rbtree_impl> impl_;
      size_t bound_ = 0;
      uint32_t pos_ = 0;

     public:
      explicit InnerIterator(std::shared_ptr<rbtree_impl> impl) : impl_(impl) {
        bound_ = impl_->MemUsed() >> padding;
      }

      void Next() {
        Lock lock(&impl_->mutex_);
        do {
          pos_ =
              threaded_rbtree_move_next(pos_, my_deref_node_t{&impl_->data_});
        } while (pos_ + 1 > bound_);
      }

      void Prev() {
        Lock lock(&impl_->mutex_);
        do {
          pos_ =
              threaded_rbtree_move_prev(pos_, my_deref_node_t{&impl_->data_});
        } while (pos_ + 1 > bound_);
      }

      void Seek(const Slice &key, const InternalKeyComparator *icmp) {
        auto deref_key = my_deref_key_t{&impl_->data_};
        auto cmp = my_cmp_t{deref_key, icmp};
        {
          Lock lock(&impl_->mutex_);
          pos_ = threaded_rbtree_lower_bound(impl_->root_,
                                             my_deref_node_t{&impl_->data_},
                                             key, deref_key, cmp);
        }
        if (pos_ + 1 > bound_) Next();
      }

      void SeekForPrev(const Slice &key, const InternalKeyComparator *icmp) {
        auto deref_key = my_deref_key_t{&impl_->data_};
        auto cmp = my_cmp_t{deref_key, icmp};
        {
          Lock lock(&impl_->mutex_);
          pos_ = threaded_rbtree_reverse_lower_bound(
              impl_->root_, my_deref_node_t{&impl_->data_}, key, deref_key,
              cmp);
        }
        if (pos_ + 1 > bound_) Prev();
      }

      void SeekToFirst() {
        {
          Lock lock(&impl_->mutex_);
          pos_ = impl_->root_.get_most_left(my_deref_node_t{&impl_->data_});
        }
        if (pos_ + 1 > bound_) Next();
      }

      void SeekToLast() {
        {
          Lock lock(&impl_->mutex_);
          pos_ = impl_->root_.get_most_right(my_deref_node_t{&impl_->data_});
        }
        if (pos_ + 1 > bound_) Prev();
      }

      Slice Key() const { return my_deref_node_t{&impl_->data_}(pos_).key(); }

      Slice Value() const {
        return my_deref_node_t{&impl_->data_}(pos_).value();
      }

      bool Valid() const { return pos_ != my_node_t::nil_sentinel; }
    };

    bool InsertKeyValue(const Slice &iKey, const Slice &val) {
      if (readOnly_) return false;
      bool mag = iKey.size() > 39;
      uint32_t klnln = mag ? VarintLength(iKey.size() - 39) : 0;
      uint32_t vlnln = VarintLength(val.size());
      size_t c =
          terark::align_up(9 + klnln + iKey.size() + vlnln + val.size(), 1 << padding);
      size_t pos = data_.alloc(c);
      if (pos == size_t(-1)) return false;
      size_t idx = pos >> padding;
      my_deref_node_t{&data_}(idx).set_key_value(iKey, val, klnln, vlnln, mag);
      my_stack_t stack;
      {
        ReadLock lock(&mutex_);
        bool ex = threaded_rbtree_find_path_for_unique(
            root_, stack, my_deref_node_t{&data_}, iKey, my_deref_key_t{&data_},
            my_cmp_t{my_deref_key_t{&data_}, rep_->icmp_});
        if (ex) return false;
      }
      WriteLock lock(&mutex_);
      threaded_rbtree_insert(root_, stack, my_deref_node_t{&data_}, idx);
      item_num_++;
      return true;
    }

    void Merge(rbtree_impl<padding> *ptr) {
      size_t c0 = ptr->MemUsed();
      size_t n = ptr->item_num_;
      size_t pos = data_.alloc(c0);
      my_stack_t stack;
      memcpy(data_.data() + pos, ptr->data_.data(), c0);
      for (size_t i = 0; i < n; ++i) {
        auto &node = my_deref_node_t{&data_}(pos >> padding);
        if (!threaded_rbtree_find_path_for_unique(
                root_, stack, my_deref_node_t{&data_}, node.key(),
                my_deref_key_t{&data_},
                my_cmp_t{my_deref_key_t{&data_}, rep_->icmp_}))
          threaded_rbtree_insert(root_, stack, my_deref_node_t{&data_},
                                 pos >> padding);
        else
          assert(false);
        pos += terark::align_up(node.capicity(), 1 << padding);
      }
      item_num_ += n;
    }

    template <class Lock>
    bool Contains(const Slice &internal_key) {
      auto deref_key = my_deref_key_t{&data_};
      auto cmp = my_cmp_t{deref_key, rep_->icmp_};
      ReadLock lock(&mutex_);
      return threaded_rbtree_equal_unique(root_, my_deref_node_t{&data_},
                                          internal_key, deref_key,
                                          cmp) != my_node_t::nil_sentinel;
    }

    template <class Lock>
    InnerIterator<Lock> NewInnerIterator() {
      return InnerIterator<Lock>(shared_from_this());
    }

    void MarkReadOnly() { readOnly_ = true; }

    bool IsReadOnly() { return readOnly_; }

    uint64_t MemUsed() { return data_.size(); }

    size_t Size() { return item_num_; }

    template <class Lock>
    uint64_t ApprxNumEntries(const Slice &start_ikey, const Slice &end_ikey) {
      auto ApprxRankRatio = [this](const Slice &key) {
        auto deref_key = my_deref_key_t{&data_};
        auto cmp = my_cmp_t{deref_key, rep_->icmp_};
        return uint64_t(
            threaded_rbtree_approximate_rank_ratio(
                root_, my_deref_node_t{&data_}, key, deref_key, cmp) *
            item_num_);
      };

      Lock lock(&mutex_);
      uint64_t st = ApprxRankRatio(start_ikey);
      uint64_t ed = ApprxRankRatio(end_ikey);

      return ed > st ? ed - st : 0;
    }
  };

  using my_rbtree_t = rbtree_impl<2>;
  struct TlsItem {
    TlsItem() = default;
    TlsItem(const TlsItem &) = delete;
    TlsItem &operator=(const TlsItem &) = delete;

    my_rbtree_t *ptr = nullptr;

    ~TlsItem() {
      if (ptr != nullptr) {
        ptr->MarkReadOnly();
        ptr->Destory();
      }
    }

    void init(my_rbtree_t *p) { ptr = p; }
    my_rbtree_t *get() { return ptr; }
  };

  struct DummyLock {
    DummyLock(...) {}
  };

  std::list<std::shared_ptr<my_rbtree_t>> rbt_list_;
  terark::instance_tls<TlsItem> rbtree_ref_;
  std::atomic_bool immutable_;
  const SliceTransform *transform_;
  const MemTableRep::KeyComparator &cmp_;
  const InternalKeyComparator *icmp_;
  mutable port::RWMutex mutex_;

 public:
  explicit ThreadedRBTreeRep(const MemTableRep::KeyComparator &compare,
                             Allocator *allocator,
                             const SliceTransform *transform)
      : MemTableRep(allocator),
        transform_(transform),
        immutable_(false),
        cmp_(compare) {
    icmp_ = cmp_.icomparator();
  }

  virtual KeyHandle Allocate(const size_t len, char **buf) override {
    assert(false);
    return nullptr;
  }

  virtual void Insert(KeyHandle handle) override { assert(false); }

  char *EncodeKey(const Slice &user_key) { return nullptr; }

  template <class Lock>
  class HeapIterator : public MemTableRep::Iterator {
   private:
    std::vector<my_rbtree_t::InnerIterator<Lock>> rbt_;
    ThreadedRBTreeRep *rep_;
    const InternalKeyComparator *icmp_;
    bool forward_ = true;

   public:
    explicit HeapIterator(ThreadedRBTreeRep *rep) : rep_(rep) {
      ReadLock rep_lock(&rep->mutex_);
      icmp_ = rep_->cmp_.icomparator();
      for (auto &rbt : rep_->rbt_list_)
        rbt_.emplace_back(std::move(rbt->NewInnerIterator<Lock>()));
    }

    struct ForwardCmp {
      const InternalKeyComparator *icmp_;
      template <class iter>
      bool operator()(const iter &l, const iter &r) const {
        if (!r.Valid()) return false;
        if (!l.Valid()) return true;
        return icmp_->Compare(l.Key(), r.Key()) > 0;
      }
    };

    struct BackwardCmp {
      const InternalKeyComparator *icmp_;
      template <class iter>
      bool operator()(const iter &l, const iter &r) const {
        if (!r.Valid()) return false;
        if (!l.Valid()) return true;
        return icmp_->Compare(l.Key(), r.Key()) < 0;
      }
    };

    virtual bool Valid() const override {
      return !rbt_.empty() && rbt_.front().Valid();
    }

    virtual void Next() override {
      assert(Valid());
      if (!forward_) Seek(GetKey(), nullptr);
      assert(Valid());
      rbt_.begin()->Next();
      terark::adjust_heap_top(rbt_.begin(), rbt_.size(), ForwardCmp{icmp_});
    }

    virtual void Prev() override {
      assert(Valid());
      if (forward_) SeekForPrev(GetKey(), nullptr);
      assert(Valid());
      rbt_.begin()->Prev();
      terark::adjust_heap_top(rbt_.begin(), rbt_.size(), BackwardCmp{icmp_});
    }

    virtual void Seek(const Slice &user_key,
                      const char *memtable_key) override {
      for (auto &iter : rbt_)
        if (memtable_key != nullptr)
          iter.Seek(GetLengthPrefixedSlice(memtable_key), rep_->icmp_);
        else
          iter.Seek(user_key, rep_->icmp_);
      std::make_heap(rbt_.begin(), rbt_.end(), ForwardCmp{icmp_});
      forward_ = true;
    }

    virtual void SeekForPrev(const Slice &user_key,
                             const char *memtable_key) override {
      for (auto &iter : rbt_)
        if (memtable_key != nullptr)
          iter.SeekForPrev(GetLengthPrefixedSlice(memtable_key), rep_->icmp_);
        else
          iter.SeekForPrev(user_key, rep_->icmp_);
      std::make_heap(rbt_.begin(), rbt_.end(), BackwardCmp{icmp_});
      forward_ = false;
    }

    virtual void SeekToFirst() override {
      for (auto &iter : rbt_) iter.SeekToFirst();
      std::make_heap(rbt_.begin(), rbt_.end(), ForwardCmp{icmp_});
      forward_ = true;
    }

    virtual void SeekToLast() override {
      for (auto &iter : rbt_) iter.SeekToLast();
      std::make_heap(rbt_.begin(), rbt_.end(), BackwardCmp{icmp_});
      forward_ = false;
    }

    virtual const char *key() const override {
      assert(false);
      return nullptr;
    }

    virtual Slice GetKey() const override { return rbt_.front().Key(); }

    virtual Slice GetValue() const override { return rbt_.front().Value(); }

    virtual std::pair<Slice, Slice> GetKeyValue() const override {
      return std::pair<Slice, Slice>(rbt_.front().Key(), rbt_.front().Value());
    }
  };

  virtual void Get(const LookupKey &k, void *callback_args,
                   bool (*callback_func)(void *arg,
                                         const KeyValuePair *kv)) override {
    EncodedKeyValuePair pair;
    Slice dummy;
    if (immutable_) {
      ThreadedRBTreeRep::HeapIterator<DummyLock> iter(this);
      for (iter.Seek(dummy, k.memtable_key().data());
           iter.Valid() && callback_func(callback_args, &iter); iter.Next()) {
      }
    } else {
      ThreadedRBTreeRep::HeapIterator<ReadLock> iter(this);
      for (iter.Seek(dummy, k.memtable_key().data());
           iter.Valid() && callback_func(callback_args, &iter); iter.Next()) {
      }
    }
  }

  virtual MemTableRep::Iterator *GetIterator(Arena *arena = nullptr) override {
    if (immutable_) {
      typedef ThreadedRBTreeRep::HeapIterator<DummyLock> i_t;
      return arena ? new (arena->AllocateAligned(sizeof(i_t))) i_t(this)
                   : new i_t(this);
    } else {
      typedef ThreadedRBTreeRep::HeapIterator<ReadLock> i_t;
      return arena ? new (arena->AllocateAligned(sizeof(i_t))) i_t(this)
                   : new i_t(this);
    }
  }

  virtual bool InsertKeyValue(const Slice &internal_key,
                              const Slice &value) override {
    assert(!immutable_);
    auto &ref = rbtree_ref_.get();
    if (ref.get() == nullptr) {
      WriteLock rep_lock(&mutex_);
      rbt_list_.emplace_back(std::make_shared<my_rbtree_t>(this));
      ref.init(&**std::prev(rbt_list_.end()));
    }
    return ref.get()->InsertKeyValue(internal_key, value);
  }

  virtual void MarkReadOnly() override {
    for (auto &rbt : rbt_list_) rbt->MarkReadOnly();
    immutable_ = true;
  }

  virtual size_t ApproximateMemoryUsage() override {
    size_t ret = 0;
    for (auto &rbt : rbt_list_) ret += rbt->MemUsed();
    return ret;
  }

  virtual uint64_t ApproximateNumEntries(const Slice &start_ikey,
                                         const Slice &end_ikey) override {
    size_t n = 0;
    ReadLock rep_lock(&mutex_);
    if (immutable_)
      for (auto &rbt : rbt_list_)
        n += rbt->ApprxNumEntries<DummyLock>(start_ikey, end_ikey);
    else
      for (auto &rbt : rbt_list_)
        n += rbt->ApprxNumEntries<ReadLock>(start_ikey, end_ikey);
    return n;
  }

  virtual bool Contains(const Slice &internal_key) const override {
    ReadLock rep_lock(&mutex_);
    if (immutable_) {
      for (auto &rbt : rbt_list_)
        if (const_cast<my_rbtree_t *>(&*rbt)->Contains<DummyLock>(internal_key))
          return true;
    } else
      for (auto &rbt : rbt_list_)
        if (const_cast<my_rbtree_t *>(&*rbt)->Contains<ReadLock>(internal_key))
          return true;
    return false;
  }
};
}  // namespace

class TRBTreeRepFactory : public MemTableRepFactory {
 public:
  explicit TRBTreeRepFactory() {}

  using MemTableRepFactory::CreateMemTableRep;
  virtual MemTableRep *CreateMemTableRep(
      const MemTableRep::KeyComparator &comparator,
      Allocator *allocator, const SliceTransform *transform,
      Logger *logger) override {
    return new ThreadedRBTreeRep(comparator, allocator, transform);
  };

  virtual const char *Name() const override { return "TRBTreeRepFactory"; }

  bool IsInsertConcurrentlySupported() const override { return true; }
};

MemTableRepFactory *NewTRBTreeRepFactory() { return new TRBTreeRepFactory(); }

}  // namespace rocksdb
